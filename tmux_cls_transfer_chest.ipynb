{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['number of train images is 5670', 'number of valid images is 630', 'number of test images is 700']\n",
      "Classes with index are: {'Normal': 0, 'Tuberculosis': 1}\n",
      "['Normal', 'Tuberculosis']\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/189 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/raza.imam/Documents/HC701B/Project/tmux_cls_transfer_chest.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.16/home/raza.imam/Documents/HC701B/Project/tmux_cls_transfer_chest.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     x \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mblocks[i](x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.16/home/raza.imam/Documents/HC701B/Project/tmux_cls_transfer_chest.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# output = classifier(x[:,0,:])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.16/home/raza.imam/Documents/HC701B/Project/tmux_cls_transfer_chest.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m output \u001b[39m=\u001b[39m classifier(x[:,\u001b[39m1\u001b[39;49m:,:])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.16/home/raza.imam/Documents/HC701B/Project/tmux_cls_transfer_chest.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m prediction \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output , dim\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.16/home/raza.imam/Documents/HC701B/Project/tmux_cls_transfer_chest.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/HC701B/Project/TransferClassifier.py:34\u001b[0m, in \u001b[0;36mTransferLearningModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m patch_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(hidden_size \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m)\n\u001b[1;32m     33\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m196\u001b[39m, \u001b[39m768\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     35\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(batch_size, patch_size, patch_size \u001b[39m*\u001b[39m num_patches)\n\u001b[1;32m     36\u001b[0m \u001b[39mprint\u001b[39m((x\u001b[39m.\u001b[39mshape))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "# import functions\n",
    "# import predictions\n",
    "# from predictions import Prediction\n",
    "# from features import Features\n",
    "import mlp\n",
    "from TransferClassifier import TransferLearningModel\n",
    "from data import dataset as data\n",
    "# import attacks\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns \n",
    "import random\n",
    "import pickle\n",
    "import torch \n",
    "from sklearn.metrics import auc\n",
    "from autoattack import AutoAttack\n",
    "from torchvision.utils import save_image\n",
    "import math\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "# import train_model\n",
    "import os\n",
    "# from ensemble_whitebox import New\n",
    "import timm \n",
    "import torchsummary\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import pickle\n",
    "\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']= '4'\n",
    "\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "vit = 'models/vit_base_patch16_224_in21k_test-accuracy_0.96_chest.pth'\n",
    "image_size = (224,224)\n",
    "batch_size = 30\n",
    "\n",
    "model = torch.load(vit).cuda()\n",
    "\n",
    "for w in model.parameters(): \n",
    "    w.requires_grad = False\n",
    "\n",
    "root_dir = \"data/TB_data/\"\n",
    "data_loader, image_dataset = data.data_loader(root_dir=root_dir, batch_size= batch_size, image_size=image_size)\n",
    "\n",
    "\n",
    "test_list = {f'block_{i}': [] for i in range(12)}\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "for index in range(12):\n",
    "    classifier = TransferLearningModel(num_classes=2).cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(),lr = lr)\n",
    "    scheduler = StepLR(optimizer=optimizer, step_size=15, gamma=0.1, verbose=True)\n",
    "\n",
    "    classifier.train()\n",
    "    for epoch in range(epochs): \n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0 \n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        for image, label in tqdm(data_loader['train']):\n",
    "            image = image.cuda()\n",
    "            label = label.cuda()\n",
    "            x = model.patch_embed(image)\n",
    "            x = torch.cat((model.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "            x = model.pos_drop(x + model.pos_embed)\n",
    "            for i in range(index+1):\n",
    "                x = model.blocks[i](x)\n",
    "            # output = classifier(x[:,0,:])\n",
    "            output = classifier(x[:,1:,:])\n",
    "            prediction = torch.argmax(output , dim= -1)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            acc = sum(prediction == label).float().item()/len(label)\n",
    "            train_acc += acc \n",
    "            \n",
    "        scheduler.step() \n",
    "        train_acc = train_acc/len(data_loader['train'])   \n",
    "        train_loss = train_loss/len(data_loader['train'])\n",
    "        print(f'train_acc= {train_acc:.2f}')\n",
    "        print (f'train_loss = {train_loss:.2f}')    \n",
    "    \n",
    "    #Test loop \n",
    "    print('Testing')\n",
    "    classifier.eval()\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for image, label in tqdm(data_loader['test']):\n",
    "            image = image.cuda()\n",
    "            label = label.cuda()\n",
    "            x = model.patch_embed(image)\n",
    "            x = torch.cat((model.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "            x = model.pos_drop(x + model.pos_embed)\n",
    "            for i in range(index+1):\n",
    "                x = model.blocks[i](x)\n",
    "            output = classifier(x[:,1,:])\n",
    "            loss = criterion(output, label)\n",
    "            acc = sum(torch.argmax(output, dim=-1) == label).float().item()/len(label)\n",
    "            test_acc += acc\n",
    "            test_loss += loss.item()\n",
    "    test_acc = test_acc/len(data_loader['test'])\n",
    "    test_loss = test_loss/len(data_loader['test'])\n",
    "    print(f'test_acc = {test_acc:.2f}')\n",
    "    print (f'test_loss = {test_loss:.2f}')\n",
    "    test_list[f'block_{index}'].append(test_acc)\n",
    "    print(f'================= Block {index} Finished ==============')\n",
    "\n",
    "with open(\"accuracy_list_chest_mlp_cls_latest\", \"wb\") as fp:   #Pickling\n",
    "     pickle.dump(test_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
