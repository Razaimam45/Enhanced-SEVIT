{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: 512 x 512 x 3\n"
     ]
    }
   ],
   "source": [
    "path = '/home/raza.imam/Documents/HC701B/Project/data/TB_data/testing/Normal/Normal-4.png'\n",
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "img = cv2.imread(path)\n",
    "\n",
    "# Get the shape of the image\n",
    "height, width, channels = img.shape\n",
    "\n",
    "print(f\"Image shape: {height} x {width} x {channels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "from data.dataset import data_loader, data_loader_attacks\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mlp\n",
    "\n",
    "def majority_voting(data_loader, model, mlps_list):\n",
    "    \"\"\"\n",
    "    SEViT performance with majority voting. \n",
    "\n",
    "    Args: \n",
    "    data_loader: loader of test samples for clean images, or attackes generated from the test samples\n",
    "    model: ViT model \n",
    "    mlps_list: list of intermediate MLPs\n",
    "\n",
    "    Return: \n",
    "    Accuracy. \n",
    "    \"\"\"\n",
    "\n",
    "    acc_ = 0.0 \n",
    "    for images, labels in tqdm(data_loader):\n",
    "        final_prediction = []\n",
    "        images = images.cuda()\n",
    "        vit_output = model(images)\n",
    "        vit_predictions = torch.argmax(vit_output.detach().cpu(), dim=-1)\n",
    "        final_prediction.append(vit_predictions.detach().cpu())\n",
    "        x = model.patch_embed(images)\n",
    "        x_0 = model.pos_drop(x)\n",
    "        i=0\n",
    "        for mlp in mlps_list:\n",
    "            x_0 = model.blocks[i](x_0)\n",
    "            mlp_output = mlp(x_0)\n",
    "            mlp_predictions = torch.argmax(mlp_output.detach().cpu(), dim=-1)\n",
    "            final_prediction.append(mlp_predictions.detach().cpu())\n",
    "            i+=1\n",
    "        stacked_tesnor = torch.stack(final_prediction,dim=1)\n",
    "        preds_major = torch.argmax(torch.nn.functional.one_hot(stacked_tesnor).sum(dim=1), dim=-1)\n",
    "        acc = (preds_major == labels).sum().item()/len(labels)\n",
    "        acc_ += acc\n",
    "    final_acc = acc_ / len(data_loader)\n",
    "    print(f'Final Accuracy From Majority Voting = {(final_acc *100) :.3f}%' )\n",
    "    return final_acc\n",
    "\n",
    "def countParams(model):\n",
    "    total_params = sum(param.numel() for param in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Total Params: \", total_params)\n",
    "    print(\"Trainable Params: \", trainable_params)\n",
    "\n",
    "def vote(images_type, mlp_path):\n",
    "    root_dir = 'data/TB_data/'\n",
    "    vit_path = 'models/vit_base_patch16_224_in21k_test-accuracy_0.96_chest.pth'\n",
    "    # mlp_path = 'models/MLP_new_chest/'\n",
    "\n",
    "    model = torch.load(vit_path).cuda()\n",
    "    model.eval()\n",
    "    print('ViT is loaded!')\n",
    "\n",
    "    MLPs_list = get_classifiers_list(MLP_path=mlp_path, num_classifiers=5)\n",
    "    print('All MLPs are loaded!')\n",
    "\n",
    "    if images_type == 'clean':\n",
    "        image_folder_path = 'data/TB_data'\n",
    "        loader_, dataset_ = data_loader(root_dir=image_folder_path, batch_size=15)\n",
    "        majority_voting(data_loader=loader_['test'], model= model, mlps_list=MLPs_list)\n",
    "        # average(data_loader=loader_['test'], model= model, mlps_list=MLPs_list)\n",
    "        # weighted_average(data_loader=loader_['test'], model= model, mlps_list=MLPs_list)\n",
    "    elif images_type == 'attack':\n",
    "        image_folder_path = 'attack_images'\n",
    "        attack_name = 'AUTOPGD'\n",
    "        loader_, dataset_ = data_loader_attacks(root_dir=image_folder_path, attack_name= attack_name, batch_size=15)\n",
    "        majority_voting(data_loader=loader_, model= model, mlps_list=MLPs_list)\n",
    "        # average(data_loader=loader_, model= model, mlps_list=MLPs_list)\n",
    "        # weighted_average(data_loader=loader_, model= model, mlps_list=MLPs_list)\n",
    "    else:\n",
    "        print('Input correct image type')\n",
    "\n",
    "# path = 'ReVIT/models/cnns/'\n",
    "# vote('attack', path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before adversaring Training. Table 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Clean Performance of MLP alternatives, from m=1 to m=5\n",
    "\n",
    "\"\"\" def Table1(images_type, mlp_path):\n",
    "    root_dir = 'data/TB_data/'\n",
    "    vit_path = 'models/vit_base_patch16_224_in21k_test-accuracy_0.96_chest.pth'\n",
    "\n",
    "    model = torch.load(vit_path).cuda()\n",
    "    model.eval()\n",
    "    print('ViT is loaded!')\n",
    "\n",
    "    MLPs_list = get_classifiers_list(MLP_path=mlp_path, num_classifiers=5)\n",
    "    print('All MLPs are loaded!')\n",
    "    print(\"# of parameters of a module:\")\n",
    "    countParams(MLPs_list[0])\n",
    "\n",
    "    if images_type == 'clean':\n",
    "        image_folder_path = 'data/TB_data'\n",
    "        loader_, dataset_ = data_loader(root_dir=image_folder_path, batch_size=15)\n",
    "        for i in range(0, 6): # 0=VIT, 1 to 5 = mlp modules\n",
    "            MLPs_subset = MLPs_list[:i]\n",
    "            print(len(MLPs_subset))\n",
    "            print(f\"For m = {i}\")        \n",
    "            majority_voting(data_loader=loader_['test'], model= model, mlps_list=MLPs_subset)\n",
    "            average(data_loader=loader_['test'], model= model, mlps_list=MLPs_subset)\n",
    "            weighted_average(data_loader=loader_['test'], model= model, mlps_list=MLPs_subset)\n",
    "    elif images_type == 'attack':\n",
    "        image_folder_path = 'attack_images'\n",
    "        attack_name = 'AUTOPGD'\n",
    "        loader_, dataset_ = data_loader_attacks(root_dir=image_folder_path, attack_name= attack_name, batch_size=15)\n",
    "        majority_voting(data_loader=loader_, model= model, mlps_list=MLPs_list)\n",
    "        average(data_loader=loader_, model= model, mlps_list=MLPs_list)\n",
    "        weighted_average(data_loader=loader_, model= model, mlps_list=MLPs_list)\n",
    "    else:\n",
    "        print('Input correct image type')\n",
    "\n",
    "path_list = ['ReVIT/models/cnns', 'ReVIT/models/resnets_ft', 'ReVIT/models/resnets_tl', 'ReVIT/models/resnets_ft_cnn', 'ReVIT/models/resnets_tl_cnn', 'ReVIT/models/mlps']\n",
    "for i in range(0, 6):\n",
    "    path = path_list[i]\n",
    "    print(\"For\", path.split('/')[-1])\n",
    "    Table1('clean', path)\n",
    "\n",
    "# MLPs_list = get_classifiers_list(MLP_path=path, num_classifiers=5)\n",
    "# print(MLPs_list)\n",
    "# print('All MLPs are loaded!')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Testing__ Diffusion samples on SEVIT of MLP ensmbles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-->>> For MLP_new_chest\n",
      "['number of train images is 2', 'number of valid images is 2', 'number of test images is 698']\n",
      "Classes with index are: {'Normal': 0, 'Tuberculosis': 1}\n",
      "['Normal', 'Tuberculosis']\n",
      "ViT is loaded!\n",
      "mlp_block_0_classifier_0.94test_0.98train.pth\n",
      "MLP 1 is loaded!\n",
      "mlp_block_1_classifier_0.93test_0.99train.pth\n",
      "MLP 2 is loaded!\n",
      "mlp_block_2_classifier_0.94test_0.99train.pth\n",
      "MLP 3 is loaded!\n",
      "mlp_block_3_classifier_0.93test_0.99train.pth\n",
      "MLP 4 is loaded!\n",
      "mlp_block_4_classifier_0.92test_1.00train.pth\n",
      "All MLPs are loaded!\n",
      "# of parameters of a module:\n",
      "Total Params:  625219970\n",
      "Trainable Params:  625219970\n",
      "0\n",
      "For m = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VisionTransformer' object has no attribute 'no_embed_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodels/MLP_new_chest\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-->>> For\u001b[39m\u001b[39m\"\u001b[39m, path\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m Table2(\u001b[39m'\u001b[39;49m\u001b[39mclean\u001b[39;49m\u001b[39m'\u001b[39;49m, path)\n",
      "\u001b[1;32m/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb Cell 5\u001b[0m in \u001b[0;36mTable2\u001b[0;34m(images_type, mlp_path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(MLPs_subset))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor m = \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)        \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         majority_voting(data_loader\u001b[39m=\u001b[39;49mloader_[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m], model\u001b[39m=\u001b[39;49m model, mlps_list\u001b[39m=\u001b[39;49mMLPs_subset)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39m# average(data_loader=loader_['test'], model= model, mlps_list=MLPs_subset)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39m# weighted_average(data_loader=loader_['test'], model= model, mlps_list=MLPs_subset)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInput correct image type\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb Cell 5\u001b[0m in \u001b[0;36mmajority_voting\u001b[0;34m(data_loader, model, mlps_list)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m final_prediction \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m vit_output \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m vit_predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(vit_output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu(), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.18/home/raza.imam/Documents/HC701B/Project/voting_diffusion.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m final_prediction\u001b[39m.\u001b[39mappend(vit_predictions\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/timm/models/vision_transformer.py:549\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 549\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[1;32m    550\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_head(x)\n\u001b[1;32m    551\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/timm/models/vision_transformer.py:533\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_features\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    532\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_embed(x)\n\u001b[0;32m--> 533\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pos_embed(x)\n\u001b[1;32m    534\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_pre(x)\n\u001b[1;32m    535\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_checkpointing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/timm/models/vision_transformer.py:517\u001b[0m, in \u001b[0;36mVisionTransformer._pos_embed\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_pos_embed\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 517\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mno_embed_class:\n\u001b[1;32m    518\u001b[0m         \u001b[39m# deit-3, updated JAX (big vision)\u001b[39;00m\n\u001b[1;32m    519\u001b[0m         \u001b[39m# position embedding does not overlap with class token, add then concat\u001b[39;00m\n\u001b[1;32m    520\u001b[0m         x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed\n\u001b[1;32m    521\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VisionTransformer' object has no attribute 'no_embed_class'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Table X: Attack Performance (Diffusion Samples) of MLP ensembles, from m=1 to m=4\n",
    "import six\n",
    "def Table2(images_type, mlp_path):\n",
    "    if images_type == 'clean':\n",
    "        diff_dir = 'data/Diffusion_TB_data/sanity_check_blr'\n",
    "        image_folder_path = diff_dir\n",
    "        loader_, dataset_ = data_loader(root_dir=image_folder_path, batch_size=15)\n",
    "        \n",
    "        vit_path = 'models/vit_base_patch16_224_in21k_test-accuracy_0.96_chest.pth'\n",
    "        model = torch.load(vit_path).cuda()\n",
    "        model.eval()\n",
    "        print('ViT is loaded!')\n",
    "\n",
    "        MLPs_list = get_classifiers_list(MLP_path=mlp_path, num_classifiers=4)\n",
    "        print('All MLPs are loaded!')\n",
    "        print(\"# of parameters of a module:\")\n",
    "        countParams(MLPs_list[0])\n",
    "    \n",
    "        for i in range(0, 6): # 0=VIT, 1 to 5 = mlp modules\n",
    "            MLPs_subset = MLPs_list[:i]\n",
    "            print(len(MLPs_subset))\n",
    "            print(f\"For m = {i}\")        \n",
    "            majority_voting(data_loader=loader_['test'], model= model, mlps_list=MLPs_subset)\n",
    "            # average(data_loader=loader_['test'], model= model, mlps_list=MLPs_subset)\n",
    "            # weighted_average(data_loader=loader_['test'], model= model, mlps_list=MLPs_subset)\n",
    "    else:\n",
    "        print('Input correct image type')\n",
    "\n",
    "\n",
    "path = 'models/MLP_new_chest'\n",
    "print(\"\\n\\n-->>> For\", path.split('/')[-1])\n",
    "Table2('clean', path)\n",
    "\n",
    "# MLPs_list = get_classifiers_list(MLP_path=path, num_classifiers=5)\n",
    "# print(MLPs_list)\n",
    "# print('All MLPs are loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2.1: Attack Performance (on adversarial examples generated for the surrogate model) of MLP alternatives, from m=1 to m=4\n",
    "# Perturbation Budget is 0.03\n",
    "\"\"\"\n",
    "def Table2_1(images_type, mlp_path):\n",
    "    # root_dir = 'data/TB_data/'\n",
    "    vit_path = 'models/vit_base_patch16_224_in21k_test-accuracy_0.96_chest.pth'\n",
    "\n",
    "    model = torch.load(vit_path).cuda()\n",
    "    model.eval()\n",
    "    print('ViT is loaded!')\n",
    "\n",
    "    MLPs_list = get_classifiers_list(MLP_path=mlp_path, num_classifiers=4)\n",
    "    print('All MLPs are loaded!')\n",
    "    print(\"# of parameters of a module:\")\n",
    "    countParams(MLPs_list[0])\n",
    "\n",
    "    if images_type == 'attack':\n",
    "        image_folder_path = 'attack_images_surrogate_0.03'\n",
    "        attack_list = ['FGSM', 'PGD', 'BIM', 'AUTOPGD', 'CW']\n",
    "        for i in attack_list: #For all attack types in attack_list\n",
    "            attack_name = i\n",
    "            print(f'\\nFor {attack_name} Attack samples:')\n",
    "            loader_, dataset_ = data_loader_attacks(root_dir=image_folder_path, attack_name= attack_name, batch_size=15)\n",
    "            for i in range(1, 5):\n",
    "                MLPs_subset = MLPs_list[:i]\n",
    "                print(len(MLPs_subset))\n",
    "                print(f\"For m = {i}\")        \n",
    "                majority_voting(data_loader=loader_, model= model, mlps_list=MLPs_subset)\n",
    "                average(data_loader=loader_, model= model, mlps_list=MLPs_subset)\n",
    "                weighted_average(data_loader=loader_, model= model, mlps_list=MLPs_subset)\n",
    "    else:\n",
    "        print('Input correct image type')\n",
    "\n",
    "path_list = ['ReVIT/models/cnns', 'ReVIT/models/resnets_ft', 'ReVIT/models/resnets_tl', 'ReVIT/models/resnets_ft_cnn', 'ReVIT/models/resnets_tl_cnn', 'ReVIT/models/mlps']\n",
    "for i in range(0, 6):\n",
    "    path = path_list[i]\n",
    "    print(\"\\n\\n-->>> For\", path.split('/')[-1])\n",
    "    Table2_1('attack', path)\n",
    "\n",
    "# MLPs_list = get_classifiers_list(MLP_path=path, num_classifiers=5)\n",
    "# print(MLPs_list)\n",
    "# print('All MLPs are loaded!')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89ade8f41426f99352904e8be2cf333ad690b99e026c88e03d1f284bef19dc61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
